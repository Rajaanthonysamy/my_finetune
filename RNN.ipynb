{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJoYiUP/+yUIw9wMy74gG8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rajaanthonysamy/my_finetune/blob/main/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recurrent Neural Networks (RNNs) with an Example\n",
        "\n",
        "Imagine you have the sentence: \"The movie was fantastic!\"\n",
        "\n",
        "An RNN processes this sentence word by word, maintaining an internal 'memory' (hidden state) that gets updated with each new word. This memory allows the network to understand context.\n",
        "\n",
        "Here's how it would conceptually work for sentiment analysis:\n",
        "\n",
        "1.  **\"The\"**: The RNN processes \"The\". It might generate a hidden state `h1` representing its understanding so far. (Initial state: `h0`)\n",
        "\n",
        "    *Pictorial:* `h0` -> \"The\" -> `h1`\n",
        "\n",
        "2.  **\"movie\"**: Next, it processes \"movie\", taking `h1` into account. It updates its hidden state to `h2`.\n",
        "\n",
        "    *Pictorial:* `h1` -> \"movie\" -> `h2`\n",
        "\n",
        "3.  **\"was\"**: It processes \"was\", considering `h2`, and updates its state to `h3`.\n",
        "\n",
        "    *Pictorial:* `h2` -> \"was\" -> `h3`\n",
        "\n",
        "4.  **\"fantastic!\"**: Finally, it processes \"fantastic!\", using `h3`. The word \"fantastic!\" is crucial for sentiment. The hidden state becomes `h4`.\n",
        "\n",
        "    *Pictorial:* `h3` -> \"fantastic!\" -> `h4`\n",
        "\n",
        "After processing the entire sentence, the final hidden state `h4` contains a summary of the information from all previous words. This `h4` can then be fed into a final layer (e.g., a Dense layer with a sigmoid activation for binary classification) to predict the sentiment (e.g., \"Positive\").\n",
        "\n",
        "**Key characteristics in this flow:**\n",
        "*   **Loop**: The `h` (hidden state) feeds back into the processing of the next word, creating a 'loop' in the network diagram.\n",
        "*   **Memory**: `h_t` at any step `t` depends on `h_{t-1}` and the current input `x_t` (the word at step `t`).\n",
        "*   **Variable Length Input**: RNNs can handle sentences of different lengths because the same weights are applied at each time step.\n",
        "\n",
        "**Visual Analogy (if I could draw it):**\n",
        "Imagine a series of interconnected boxes, each representing a time step (a word). Each box receives an input (the current word) and an arrow from the previous box (the hidden state). It then produces an output (its updated hidden state) that flows to the next box, and potentially an output for that specific word. The final box in the sequence would then produce the overall prediction for the sentence."
      ],
      "metadata": {
        "id": "4lqDMItbJvUA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HQptCpZDJsQD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}